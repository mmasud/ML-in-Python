{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loan_ID               0\n",
       "Gender               13\n",
       "Married               3\n",
       "Dependents           15\n",
       "Education             0\n",
       "Self_Employed        32\n",
       "ApplicantIncome       0\n",
       "CoapplicantIncome     0\n",
       "LoanAmount           22\n",
       "Loan_Amount_Term     14\n",
       "Credit_History       50\n",
       "Property_Area         0\n",
       "Loan_Status           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the loan data\n",
    "\n",
    "#df = pd.read_csv(\"https://raw.githubusercontent.com/kiwidamien/StackedTurtles/master/content/platt_scaling/lending_club_clean_and_processed.csv\")\n",
    "\n",
    "df= pd.read_csv('G:\\\\Min enhet\\\\BackUp\\\\Downloads\\\\Datasets\\\\loan-prediction\\\\train.csv')\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition data into train and test sets\n",
    "X = df.drop(columns= ['Loan_ID', 'Loan_Status'], axis = 1) #df.drop('defaulted', axis = 1)\n",
    "y = df['Loan_Status']#df.defaulted\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N numeric_features: 5 \n",
      "\n",
      "ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term, Credit_History\n"
     ]
    }
   ],
   "source": [
    "select_numeric_features = make_column_selector(dtype_include=np.number)\n",
    "\n",
    "numeric_features = select_numeric_features(X_train)\n",
    "\n",
    "print(f'N numeric_features: {len(numeric_features)} \\n')\n",
    "print(', '.join(numeric_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the missing type to numpy missing type\n",
    "X_train.fillna(np.nan, inplace=True)\n",
    "X_test.fillna(np.nan, inplace=True)\n",
    "\n",
    "\n",
    "numeric_pipeline = make_pipeline(SimpleImputer(strategy='median', add_indicator=True))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine categorical and numerical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(exclude= [np.number]).columns\n",
    "\n",
    "# Get the skew for numeric features\n",
    "feature_skew = X_train.select_dtypes(include = [np.number]).skew()\n",
    "print(feature_skew)\n",
    "\n",
    "# Break the features into two groups: log transformation for highly skewed data , Scaling otherwise\n",
    "log_features = feature_skew[abs(feature_skew) > 0.9].index\n",
    "scale_features = [name for name in feature_skew.index if name not in log_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N skew_features: 5 \n",
      "\n",
      "ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term, Credit_History\n"
     ]
    }
   ],
   "source": [
    "# Numeric with high skew\n",
    "\n",
    "def select_skew_features(df):\n",
    "    \n",
    "    skew_features =\\\n",
    "        df\\\n",
    "        .select_dtypes(include = [np.number]).skew()\\\n",
    "        .loc[lambda x: abs(x) > 0.9]\\\n",
    "        .index\\\n",
    "        .tolist()\n",
    "        \n",
    "    return skew_features\n",
    "\n",
    "skew_features = select_skew_features(X_train)\n",
    "\n",
    "print(f'N skew_features: {len(skew_features)} \\n')\n",
    "print(', '.join(skew_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put it in a pipeline\n",
    "\n",
    "log_pipeline = make_pipeline(SimpleImputer(strategy='median'), # fill the missing values first\n",
    "                             FunctionTransformer(func= np.log1p, validate=False)) # Do log transform on skew columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale to make the values for rest of the numeric columns between 0 and 1\n",
    "scale_pipeline = make_pipeline( StandardScaler()) # Do scaling on non-skew columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical with moderate-to-low cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N oh_features: 6 \n",
      "\n",
      "Gender, Married, Dependents, Education, Self_Employed, Property_Area\n"
     ]
    }
   ],
   "source": [
    "# OH transformation may not be suitable for features with high cardinality. \n",
    "# For the sake of illustration, I'm going to set my limit at 20 values.\n",
    "\n",
    "MAX_OH_CARDINALITY = 10\n",
    "\n",
    "def select_oh_features(df):\n",
    "    \n",
    "    hc_features =\\\n",
    "        df\\\n",
    "        .select_dtypes(['object', 'category'])\\\n",
    "        .apply(lambda col: col.nunique())\\\n",
    "        .loc[lambda x: x <= MAX_OH_CARDINALITY]\\\n",
    "        .index\\\n",
    "        .tolist()\n",
    "        \n",
    "    return hc_features\n",
    "\n",
    "oh_features = select_oh_features(X_train)\n",
    "\n",
    "print(f'N oh_features: {len(oh_features)} \\n')\n",
    "print(', '.join(oh_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_pipeline = make_pipeline(SimpleImputer(strategy='constant'), OneHotEncoder(handle_unknown='ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N hc_features: 0 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Categorical with high cardinality\n",
    "\n",
    "def select_hc_features(df):\n",
    "    \n",
    "    hc_features =\\\n",
    "        df\\\n",
    "        .select_dtypes(['object', 'category'])\\\n",
    "        .apply(lambda col: col.nunique())\\\n",
    "        .loc[lambda x: x > MAX_OH_CARDINALITY]\\\n",
    "        .index\\\n",
    "        .tolist()\n",
    "        \n",
    "    return hc_features\n",
    "\n",
    "\n",
    "hc_features = select_hc_features(X_train)\n",
    "\n",
    "print(f'N hc_features: {len(hc_features)} \\n')\n",
    "print(', '.join(hc_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_pipeline = make_pipeline(ce.GLMMEncoder())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=8,\n",
       "                                   transformers=[('numeric_pipeline',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(add_indicator=True,\n",
       "                                                                                 strategy='median'))]),\n",
       "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x00000247838CFD88>),\n",
       "                                                 ('oh_pipeline',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(strategy='constant')),\n",
       "                                                                  ('onehotencoder',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  <function select_oh_features at 0x0000024783856B88>),\n",
       "                                                 ('hc_pipeline',\n",
       "                                                  Pipeline(steps=[('glmmencoder',\n",
       "                                                                   GLMMEncoder())]),\n",
       "                                                  <function select_hc_features at 0x0000024783856168>)])),\n",
       "                ('classifier', RandomForestClassifier())])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_transformer = ColumnTransformer(transformers=\\\n",
    "                                       [('numeric_pipeline', numeric_pipeline, select_numeric_features),\\\n",
    "                                        ('oh_pipeline', oh_pipeline, select_oh_features),\\\n",
    "                                        ('hc_pipeline', hc_pipeline, select_hc_features)],\n",
    "                                       n_jobs = multiprocessing.cpu_count(),\n",
    "                                       remainder='drop')\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "#model = GradientBoostingClassifier(learning_rate=0.025, n_estimators=1000, subsample=0.25, max_depth=5,\\\n",
    "#                                 min_samples_split=50, max_features='sqrt')\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', column_transformer),\n",
    "                      ('classifier', model)])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "#print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Accuracy    Recall\n",
      "Train  1.000000  1.000000\n",
      "Test   0.767568  0.966942\n"
     ]
    }
   ],
   "source": [
    "out_df = pd.DataFrame([\n",
    "    [clf.score(X_train, y_train), recall_score(y_train.values, clf.predict(X_train), pos_label='Y')], \n",
    "    # pos_label: indicate which label is the positive one \n",
    "    [clf.score(X_test, y_test), recall_score(y_test, clf.predict(X_test), pos_label='Y')],\n",
    "], columns = ['Accuracy', 'Recall'], index=['Train', 'Test'])\n",
    "\n",
    "print(out_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the prediction pipeline in a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-362-1da965d72761>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m#Get the best estimator and print out the estimator model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mbest_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbest_clf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "column_transformer = ColumnTransformer(transformers=\\\n",
    "                                       [('numeric_pipeline', numeric_pipeline, select_numeric_features),\\\n",
    "                                        ('oh_pipeline', oh_pipeline, select_oh_features),\\\n",
    "                                        ('hc_pipeline', hc_pipeline, select_hc_features)],\n",
    "                                       n_jobs = multiprocessing.cpu_count(),\n",
    "                                       remainder='drop')\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "#model = GradientBoostingClassifier(learning_rate=0.025, n_estimators=1000, subsample=0.25, max_depth=5,\\\n",
    "#                                 min_samples_split=50, max_features='sqrt')\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', column_transformer),\n",
    "                      ('clf', model)])\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=5, random_state=22)\n",
    "\n",
    "\n",
    "#Create the parameter grid, entering the values to use for each parameter selected in the RandomForest estimator\n",
    "parameters = {'clf__n_estimators': [50, 100, 200, 500, 1000], \n",
    "              'clf__max_features': ['log2', 'sqrt','auto'],\n",
    "              'clf__criterion': ['entropy', 'gini'], \n",
    "              'clf__max_depth': [2, 3, 5, 9], \n",
    "              'clf__min_samples_split': [2, 3, 5],\n",
    "              'clf__min_samples_leaf': [1,5,8] \n",
    "             }\n",
    "\n",
    "grid_RF = GridSearchCV(clf, param_grid=parameters , cv= kfold)\n",
    "\n",
    "#Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_RF= grid_RF.fit(X_train, y_train)\n",
    "\n",
    "#Get the best estimator and print out the estimator model\n",
    "best_clf = grid_RF.best_estimator_\n",
    "print (best_clf)\n",
    "\n",
    "#Use best estimator to make predictions on the test set\n",
    "best_predictions = best_clf.predict(X_test)\n",
    "\n",
    "cv_result = cross_val_score(clf, X_train, y_train, cv = kfold,scoring = \"accuracy\")\n",
    "\n",
    "print(cv_result.mean())\n",
    "\n",
    "print((\"best random forest from grid search: %.3f\"\n",
    "       % best_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8043228454172366\n"
     ]
    }
   ],
   "source": [
    "cv_result = cross_val_score(clf, X_train, y_train, cv = kfold,scoring = \"accuracy\")\n",
    "\n",
    "print(cv_result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.773\n"
     ]
    }
   ],
   "source": [
    "# define the data preparation for the columns\n",
    "\n",
    "\n",
    "column_transformer = ColumnTransformer(transformers=\\\n",
    "                                       [('numeric_pipeline', numeric_pipeline, select_numeric_features),\\\n",
    "                                        ('scale_pipeline', scale_pipeline, scale_features ),\n",
    "                                        ('log_pipeline', log_pipeline, skew_features),\n",
    "                                        ('oh_pipeline', oh_pipeline, select_oh_features),\\\n",
    "                                        ('hc_pipeline', hc_pipeline, select_hc_features)],\n",
    "                                       n_jobs = multiprocessing.cpu_count(),\n",
    "                                       remainder='drop') # or, remainder='passthrough') # ColumnTransformer will move all features that were not transformed, but this can be overriden by using the remainder=\"passthrough\"\n",
    "\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "#model = GradientBoostingClassifier(learning_rate=0.025, n_estimators=1000, subsample=0.25, max_depth=5,\\\n",
    "#                                 min_samples_split=50, max_features='sqrt')\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', column_transformer),\n",
    "                      ('classifier', model)])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(n_jobs=8,\n",
       "                  transformers=[('numeric_pipeline',\n",
       "                                 Pipeline(steps=[('simpleimputer',\n",
       "                                                  SimpleImputer(add_indicator=True,\n",
       "                                                                strategy='median'))]),\n",
       "                                 <sklearn.compose._column_transformer.make_column_selector object at 0x00000247838CFD88>),\n",
       "                                ('scale_pipeline',\n",
       "                                 Pipeline(steps=[('standardscaler',\n",
       "                                                  StandardScaler())]),\n",
       "                                 []),\n",
       "                                ('log_pipeline',\n",
       "                                 Pipeline(steps=[('funct...\n",
       "                                  'LoanAmount', 'Loan_Amount_Term',\n",
       "                                  'Credit_History']),\n",
       "                                ('oh_pipeline',\n",
       "                                 Pipeline(steps=[('simpleimputer',\n",
       "                                                  SimpleImputer(strategy='constant')),\n",
       "                                                 ('onehotencoder',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                 <function select_oh_features at 0x0000024783856B88>),\n",
       "                                ('hc_pipeline',\n",
       "                                 Pipeline(steps=[('glmmencoder',\n",
       "                                                  GLMMEncoder())]),\n",
       "                                 <function select_hc_features at 0x0000024783856168>)])"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
